{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diabetes prediction: decision tree\n",
    "\n",
    "## Notebook set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell imports all necessary libraries for our diabetes prediction project:\n",
    "- **Standard libraries**: `os`, `pickle`, `pathlib` for file operations\n",
    "- **Data manipulation**: `pandas`, `numpy` for data handling and numerical operations\n",
    "- **Visualization**: `matplotlib`, `seaborn` for creating plots and charts\n",
    "- **Machine learning**: `scikit-learn` components for preprocessing, modeling, and evaluation\n",
    "- **Project modules**: Custom configuration and utility functions\n",
    "\n",
    "These imports provide the foundation for data loading, exploration, preprocessing, model training, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python standard library imports\n",
    "import os\n",
    "import pickle\n",
    "from itertools import combinations\n",
    "from pathlib import Path\n",
    "\n",
    "# PyPI imports - data manipulation and visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# PyPI imports - statistical and machine learning libraries\n",
    "from scipy.stats import randint, uniform, loguniform\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Internal imports - project-specific modules\n",
    "import configuration as config\n",
    "import functions as funcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data loading\n",
    "\n",
    "### 1.1. Load data from URL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by loading the diabetes dataset directly from a GitHub repository URL. The dataset contains medical measurements and diabetes outcomes for patients. After loading, we remove any duplicate rows to ensure data quality and reset the index for clean row numbering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load diabetes dataset from GitHub repository\n",
    "url = 'https://raw.githubusercontent.com/4GeeksAcademy/decision-tree-project-tutorial/main/diabetes.csv'\n",
    "data_df = pd.read_csv(url)\n",
    "\n",
    "# Remove any duplicate rows and reset the index\n",
    "data_df.drop_duplicates().reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Save a local copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save a local copy of the raw data in Parquet format for several reasons:\n",
    "- **Efficiency**: Parquet files are compressed and load faster than CSV\n",
    "- **Reproducibility**: Having a local copy ensures we work with the same data even if the remote source changes\n",
    "- **Organization**: Storing raw data separately from processed data maintains a clean data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory structure for raw data storage\n",
    "Path('../data/raw').mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Save a local copy of the raw data in Parquet format for efficient storage\n",
    "data_df.to_parquet(config.RAW_DATA_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Inspect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the structure and basic properties of our dataset to understand what we're working with. The `.head()` method shows the first few rows, while `.info()` provides details about data types, null values, and memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. EDA\n",
    "### 2.1. Data composition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding the distribution of each feature is crucial for identifying potential data quality issues and understanding the characteristics of our patient population. We create histograms for all 8 features to visualize:\n",
    "- **Shape of distributions**: Are they normal, skewed, or have multiple peaks?\n",
    "- **Outliers**: Are there extreme values that might need attention?\n",
    "- **Zeros**: Some medical measurements shouldn't naturally be zero, indicating missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of feature columns for analysis\n",
    "features = ['Pregnancies','Glucose','BloodPressure','SkinThickness',\n",
    "            'Insulin','BMI','DiabetesPedigreeFunction','Age']\n",
    "\n",
    "# Create subplot grid for feature histograms (2 rows, 4 columns)\n",
    "fig, axs = plt.subplots(2,4, figsize=(10,5))\n",
    "\n",
    "# Flatten to make indexing easier\n",
    "axs = axs.flatten()\n",
    "\n",
    "fig.suptitle('Feature distributions')\n",
    "\n",
    "# Plot histogram for each feature\n",
    "for i, feature in enumerate(features):\n",
    "    axs[i].hist(data_df[feature], color='black', bins=30)\n",
    "    axs[i].set_xlabel(feature)\n",
    "    axs[i].set_ylabel('Patients')\n",
    "\n",
    "# Adjust layout to prevent overlapping\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Feature interactions\n",
    "\n",
    "#### 2.2.1. Feature cross-correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature correlations help us understand relationships between different medical measurements. Strong correlations might indicate:\n",
    "- **Redundant features**: Two features measuring similar things\n",
    "- **Biological relationships**: Expected medical correlations (e.g., BMI and skin thickness)\n",
    "- **Multicollinearity**: Which could affect some machine learning algorithms\n",
    "\n",
    "We create scatter plots for all possible feature pairs to visually identify these relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate all possible pairs of features for correlation analysis\n",
    "feature_pairs = sorted(map(sorted, combinations(set(features), 2)))\n",
    "rows = len(feature_pairs) // 4  # Calculate number of rows needed\n",
    "\n",
    "# Create subplot grid for scatter plots\n",
    "fig, axs = plt.subplots(rows, 4, figsize=(10, rows*2.25))\n",
    "\n",
    "# Flatten to make indexing easier\n",
    "axs = axs.flatten()\n",
    "\n",
    "fig.suptitle('Feature correlations')\n",
    "\n",
    "# Create scatter plot for each feature pair\n",
    "for i, feature_pair in enumerate(feature_pairs):\n",
    "    axs[i].scatter(\n",
    "        data_df[feature_pair[0]],\n",
    "        data_df[feature_pair[1]],\n",
    "        color='black',\n",
    "        s=0.5  # Small point size for better visualization\n",
    "    )\n",
    "    axs[i].set_xlabel(feature_pair[0])\n",
    "    axs[i].set_ylabel(feature_pair[1])\n",
    "\n",
    "# Adjust layout to prevent overlapping\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2. Feature-label interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This analysis reveals how each feature differs between diabetic and non-diabetic patients. Boxplots show the distribution of each feature grouped by diabetes outcome, helping us identify:\n",
    "- **Discriminative features**: Which measurements best separate the two classes\n",
    "- **Feature importance**: Features with clear differences between groups will likely be important for prediction\n",
    "- **Class overlap**: How much the distributions overlap (more overlap = harder to classify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplot grid for boxplots (2 rows, 4 columns)\n",
    "fig, axs = plt.subplots(2,4, figsize=(10,5))\n",
    "\n",
    "# Flatten to make indexing easier\n",
    "axs = axs.flatten()\n",
    "\n",
    "fig.suptitle('Feature distributions by outcome')\n",
    "\n",
    "# Create boxplot for each feature, grouped by diabetes outcome\n",
    "for i, feature in enumerate(features):\n",
    "    \n",
    "    # Use seaborn boxplot to show distribution differences between outcome classes\n",
    "    sns.boxplot(data_df, x='Outcome', y=feature, ax=axs[i])\n",
    "    axs[i].set_title(feature)\n",
    "\n",
    "# Adjust layout to prevent overlapping\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Medical datasets often have missing values encoded as zeros, since measurements like blood pressure or BMI cannot naturally be zero in living patients. We analyze the frequency of zero values across features to identify which ones likely contain missing data that needs imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the fraction of zero values for each feature in training data\n",
    "zero_fraction = (data_df[features] == 0).astype(int).sum(axis=0) / len(data_df)\n",
    "zero_fraction.sort_values(inplace=True)             # Sort from lowest to highest\n",
    "zero_fraction = zero_fraction[zero_fraction > 0.0]  # Keep only features with zeros\n",
    "\n",
    "# Visualize the fraction of zero values by feature\n",
    "plt.title('Fraction zeros by feature')\n",
    "plt.bar(zero_fraction.index, zero_fraction, color='black')\n",
    "plt.xticks(rotation=25)  # Rotate x-axis labels for better readability\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Fraction zeros')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the analysis above, we select features with significant zero values for imputation. These features represent medical measurements that cannot realistically be zero, so we'll use K-Nearest Neighbors imputation to replace these missing values with estimates based on similar patients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose which features to apply imputation to\n",
    "imputed_features = ['Glucose','BMI','BloodPressure','SkinThickness','Insulin']\n",
    "\n",
    "# Save the list of imputed features\n",
    "with open(config.IMPUTED_FEATURES_FILE, 'wb') as output_file:\n",
    "    pickle.dump(imputed_features, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data preparation\n",
    "\n",
    "### 3.1. Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our final feature set for modeling. All 8 features are medical measurements that could potentially indicate diabetes risk:\n",
    "- **Pregnancies**: Number of pregnancies (diabetes can develop during pregnancy)\n",
    "- **Glucose**: Blood glucose level (primary diabetes indicator)\n",
    "- **Blood Pressure**: Diastolic blood pressure\n",
    "- **Skin Thickness**: Triceps skin fold thickness\n",
    "- **Insulin**: Serum insulin level\n",
    "- **BMI**: Body mass index (weight/height²)\n",
    "- **Diabetes Pedigree Function**: Genetic predisposition score\n",
    "- **Age**: Patient age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the final feature set for modeling\n",
    "features = ['Pregnancies','Glucose','BloodPressure','SkinThickness',\n",
    "            'Insulin','BMI','DiabetesPedigreeFunction','Age']\n",
    "\n",
    "# Define the target variable\n",
    "label = ['Outcome']\n",
    "\n",
    "# Keep only the selected features and target variable in both datasets\n",
    "data_df = data_df[features + label]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Test-train split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split our data into training and testing sets to evaluate model performance on unseen data. Key aspects:\n",
    "- **50-50 split**: Half for training, half for testing\n",
    "- **Stratified split**: Maintains the same proportion of diabetic vs non-diabetic patients in both sets\n",
    "- **Random seed**: Ensures reproducible results across runs\n",
    "- **Purpose**: Training set builds the model, testing set provides unbiased performance evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets (50-50 split)\n",
    "training_df, testing_df = train_test_split(\n",
    "    data_df,\n",
    "    test_size=0.5,                    # Use half the data for testing\n",
    "    random_state=config.RANDOM_SEED,  # Ensure reproducible results\n",
    "    stratify=data_df['Outcome']       # Maintain class distribution in splits\n",
    ")\n",
    "\n",
    "# Display the size of each split\n",
    "print(f'Training set: {len(training_df)} rows')\n",
    "print(f'Testing set: {len(testing_df)} rows')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the split datasets for use in other notebooks. This ensures:\n",
    "- **Consistency**: All notebooks use the exact same training/testing split\n",
    "- **Efficiency**: Other notebooks can skip data preparation steps\n",
    "- **Reproducibility**: Results are comparable across different experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory for processed data if it doesn't exist\n",
    "Path('../data/processed').mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Package training and testing data together for easy loading in other notebooks\n",
    "data = {\n",
    "    'training': training_df,\n",
    "    'testing': testing_df\n",
    "}\n",
    "\n",
    "# Save processed data as pickled dictionary\n",
    "with open(config.DATA_FILE, 'wb') as output_file:\n",
    "    pickle.dump(data, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we begin the machine learning phase. We'll train and evaluate several models:\n",
    "1. **Baseline models**: Simple benchmarks to establish minimum performance\n",
    "2. **Decision tree**: Our main focus - a interpretable tree-based classifier\n",
    "3. **Hyperparameter optimization**: Fine-tune the decision tree for best performance\n",
    "\n",
    "We track cross-validation scores for all models to enable fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionary to store cross-validation scores for model comparison\n",
    "cross_val_scores = {\n",
    "    'Model': [],  # Model names\n",
    "    'Score': []   # Accuracy scores\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Baseline\n",
    "\n",
    "#### 4.1.1. Constant '0' model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our simplest baseline: always predict \"no diabetes\" (the majority class). This represents the accuracy we'd get by making the most common prediction for every patient. Any useful model must significantly outperform this naive approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy for a naive baseline that always predicts \"no diabetes\" (class 0)\n",
    "# This represents the accuracy if we simply predicted the majority class\n",
    "accuracy = ((len(training_df) - sum(training_df['Outcome'])) / len(training_df))*100\n",
    "print(f'Training accuracy of constant \"0\" model: {accuracy:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.2. Logistic regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our second baseline uses logistic regression - a simple, interpretable linear model. This pipeline includes:\n",
    "- **KNN Imputation**: Fill missing values using similar patients\n",
    "- **Class weighting**: Handle the imbalanced dataset (more non-diabetic than diabetic patients)\n",
    "- **High iteration limit**: Ensure the optimization converges to the best solution\n",
    "\n",
    "This provides a more sophisticated baseline that actually uses the feature data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_imputer = ColumnTransformer([('imputer', KNNImputer(), imputed_features)], remainder='passthrough')\n",
    "\n",
    "logistic_model = LogisticRegression(\n",
    "    max_iter=5000,                     # Increase iterations to ensure convergence\n",
    "    class_weight=config.CLASS_WEIGHT,  # Handle class imbalance\n",
    "    random_state=config.RANDOM_SEED    # Ensure reproducible results\n",
    ")\n",
    "\n",
    "linear_model = Pipeline(\n",
    "    steps=[\n",
    "        ('imputer', knn_imputer),\n",
    "        ('classifier', logistic_model)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Train the logistic regression model\n",
    "linear_model.fit(training_df.drop('Outcome', axis=1), training_df['Outcome'])\n",
    "\n",
    "# Calculate training accuracy\n",
    "accuracy = accuracy_score(linear_model.predict(training_df.drop('Outcome', axis=1)), training_df['Outcome'])*100\n",
    "print(f'Training accuracy of logistic regression: {accuracy:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation provides a more reliable estimate of model performance by:\n",
    "- **Multiple evaluations**: Testing the model on different data subsets\n",
    "- **Reducing overfitting bias**: Single train-test splits can be misleading\n",
    "- **Statistical confidence**: Mean and standard deviation show performance stability\n",
    "- **Fair comparison**: All models evaluated using the same cross-validation strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-validation to get a more robust estimate of model performance\n",
    "scores = cross_val_score(\n",
    "    linear_model,\n",
    "    training_df.drop('Outcome', axis=1),\n",
    "    training_df['Outcome'],\n",
    "    cv=config.CROSS_VAL,\n",
    "    n_jobs=-1                   # Use all available CPU cores for parallel processing\n",
    ")\n",
    "\n",
    "# Store results for later comparison\n",
    "cross_val_scores['Model'].extend(['Logistic regression']*len(scores))\n",
    "cross_val_scores['Score'].extend(scores*100)\n",
    "\n",
    "# Display cross-validation results with mean and standard deviation\n",
    "print(f'Cross-validation accuracy of logistic regression: {np.mean(scores)*100:.1f} +/- {np.std(scores)*100:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Decision tree classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we introduce our main model: the decision tree classifier. Decision trees offer several advantages:\n",
    "- **Interpretability**: Easy to understand the decision logic\n",
    "- **No assumptions**: Don't assume linear relationships like logistic regression\n",
    "- **Handle interactions**: Automatically capture complex feature interactions\n",
    "- **Class weighting**: Handle imbalanced data effectively\n",
    "\n",
    "We start with default hyperparameters to establish baseline decision tree performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a decision tree classifier with default hyperparameters\n",
    "tree_model = DecisionTreeClassifier(\n",
    "    class_weight=config.CLASS_WEIGHT,  # Handle class imbalance\n",
    "    random_state=config.RANDOM_SEED    # Ensure reproducible results\n",
    ")\n",
    "\n",
    "naive_model = Pipeline(\n",
    "    steps=[\n",
    "        ('KNN', knn_imputer),\n",
    "        ('classifier', tree_model)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Train the naive decision tree model\n",
    "naive_model.fit(training_df.drop('Outcome', axis=1), training_df['Outcome'])\n",
    "\n",
    "# Calculate training accuracy\n",
    "accuracy = accuracy_score(naive_model.predict(training_df.drop('Outcome', axis=1)), training_df['Outcome'])*100\n",
    "print(f'Training accuracy of decision tree: {accuracy:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply the same cross-validation approach to the decision tree to fairly compare its performance with logistic regression. The results will show whether the tree-based approach offers any advantage over the linear model for this diabetes prediction task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-validation on the naive decision tree\n",
    "scores = cross_val_score(\n",
    "    naive_model,\n",
    "    training_df.drop('Outcome', axis=1),\n",
    "    training_df['Outcome'],\n",
    "    cv=config.CROSS_VAL,\n",
    "    n_jobs=-1                   # Use all available CPU cores for parallel processing\n",
    ")\n",
    "\n",
    "# Store results for later comparison with other models\n",
    "cross_val_scores['Model'].extend(['Decision tree']*len(scores))\n",
    "cross_val_scores['Score'].extend(scores*100)\n",
    "\n",
    "# Display cross-validation results\n",
    "print(f'Cross-validation accuracy of decision tree: {np.mean(scores)*100:.1f} +/- {np.std(scores)*100:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hyperparameter optimization\n",
    "\n",
    "### 4.1. Hyperparameter search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter optimization can significantly improve model performance. We use randomized search to efficiently explore the hyperparameter space:\n",
    "\n",
    "- **Imputation parameters**: Control how missing values are filled\n",
    "- **Tree structure**: Depth, split criteria, minimum samples per leaf\n",
    "- **Regularization**: Prevent overfitting through complexity constraints\n",
    "\n",
    "The `%%time` magic command tracks how long this computationally intensive process takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Define the hyperparameter search space for decision tree optimization\n",
    "hyperparameters = {\n",
    "    'KNN__imputer__n_neighbors': randint(1, 3),\n",
    "    'KNN__imputer__weights': ['uniform', 'distance'],\n",
    "    'KNN__imputer__add_indicator': [True, False],\n",
    "    'classifier__criterion':['gini','entropy','log_loss'],\n",
    "    'classifier__splitter':['best','random'],\n",
    "    'classifier__max_depth':randint(1, 20),\n",
    "    'classifier__min_samples_split':randint(2, 20),\n",
    "    'classifier__min_samples_leaf':randint(1, 20),\n",
    "    'classifier__min_weight_fraction_leaf':loguniform(10**-5, 0.5),\n",
    "    'classifier__max_features':uniform(loc=0.1, scale=0.9),\n",
    "    'classifier__max_leaf_nodes':randint(2, 100),\n",
    "    'classifier__min_impurity_decrease':loguniform(10**-5, 1.0),\n",
    "    'classifier__ccp_alpha':loguniform(10**-5, 10.0)\n",
    "}\n",
    "\n",
    "# Perform randomized search over hyperparameters\n",
    "search = RandomizedSearchCV(\n",
    "    naive_model,\n",
    "    hyperparameters,\n",
    "    return_train_score=True,                 # Return training scores for analysis\n",
    "    cv=config.CROSS_VAL,                     # Use stratified shuffle split for cross-validation\n",
    "    n_jobs=-1,                               # Use all available CPU cores\n",
    "    n_iter=config.RANDOM_SEARCH_ITERATIONS,  # Number of parameter combinations to try\n",
    "    random_state=config.RANDOM_SEED,         # Ensure reproducible results\n",
    ")\n",
    "\n",
    "# Fit the search and extract best model and parameters\n",
    "search_results = search.fit(training_df.drop('Outcome', axis=1), training_df['Outcome'])\n",
    "best_model = search_results.best_estimator_\n",
    "winning_hyperparameters = search_results.best_params_\n",
    "\n",
    "# Display the best hyperparameters found\n",
    "print('Best hyperparameters:\\n')\n",
    "\n",
    "for key, value in winning_hyperparameters.items():\n",
    "    print(f' {key}: {value}')\n",
    "\n",
    "print()\n",
    "\n",
    "print(f'Run time ({os.cpu_count()} CPUs):\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Hyperparameter optimization results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We visualize the hyperparameter search results to understand how much hyperparameters matter and whether we found good solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funcs.plot_cross_validation(search_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Cross-validation of optimized model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate the optimized decision tree using the same cross-validation procedure to get a fair comparison with our baseline models. This will show us the performance improvement gained through hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-validation on the optimized decision tree model\n",
    "scores = cross_val_score(\n",
    "    best_model,\n",
    "    training_df.drop('Outcome', axis=1),\n",
    "    training_df['Outcome'],\n",
    "    cv=config.CROSS_VAL,\n",
    "    n_jobs=-1                   # Use all available CPU cores for parallel processing\n",
    ")\n",
    "\n",
    "# Store results for comparison with other models\n",
    "cross_val_scores['Model'].extend(['Optimized decision tree']*len(scores))\n",
    "cross_val_scores['Score'].extend(scores*100)\n",
    "\n",
    "# Display cross-validation results for the optimized model\n",
    "print(f'Optimized decision tree cross-validation accuracy: {np.mean(scores)*100:.1f} +/- {np.std(scores)*100:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation\n",
    "\n",
    "### 5.1. Model comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A side-by-side comparison of all models using boxplots shows:\n",
    "- **Performance distribution**: Spread of cross-validation scores\n",
    "- **Model ranking**: Which models perform best on average\n",
    "- **Consistency**: Which models are most reliable (smaller variance)\n",
    "- **Statistical significance**: Whether performance differences are meaningful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create boxplot to compare cross-validation performance across all models\n",
    "sns.boxplot(pd.DataFrame.from_dict(cross_val_scores), x='Model', y='Score')\n",
    "plt.title('Cross-validation performance comparison')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Test set performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final evaluation on the held-out test set provides the true measure of model performance on unseen data. Confusion matrices reveal:\n",
    "- **True positive rate**: How many diabetic patients were correctly identified\n",
    "- **False positive rate**: How many healthy patients were misclassified as diabetic\n",
    "- **Overall accuracy**: Total correct predictions\n",
    "- **Class-specific performance**: Which class each model predicts better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the naive and winning models on the full training set for final evaluation\n",
    "result = naive_model.fit(training_df.drop('Outcome', axis=1), training_df['Outcome'])\n",
    "result = best_model.fit(training_df.drop('Outcome', axis=1), training_df['Outcome'])\n",
    "\n",
    "# Generate confusion matrices for both random forest models on the test set\n",
    "funcs.plot_confusion_matrices(\n",
    "    models = {\n",
    "        'Logistic regression': linear_model,\n",
    "        'Random forest': naive_model,\n",
    "        'Optimized random forest': best_model\n",
    "    },\n",
    "    testing_df=testing_df\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save\n",
    "\n",
    "### 6.1. Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save our results for future analysis and comparison with other algorithms (random forests, gradient boosting, etc.). This maintains a complete record of model performance across the entire project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cross-validation scores for comparison with other algorithms\n",
    "with open(config.CROSS_VAL_SCORES_FILE, 'wb') as output_file:\n",
    "    pickle.dump(cross_val_scores, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we save the trained models and their hyperparameters for:\n",
    "- **Deployment**: Using the best model in production\n",
    "- **Reproducibility**: Recreating the exact same model later\n",
    "- **Analysis**: Comparing with other algorithms in subsequent notebooks\n",
    "- **Documentation**: Recording what worked best for this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory for model storage if it doesn't exist\n",
    "Path('../models').mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Save the optimal hyperparameters for future use or reproduction\n",
    "with open(config.DECISION_TREE_HYPERPARAMETERS, 'wb') as output_file:\n",
    "    pickle.dump(winning_hyperparameters, output_file)\n",
    "\n",
    "# Save the trained best model for deployment or further analysis\n",
    "with open(config.DECISION_TREE_MODEL, 'wb') as output_file:\n",
    "    pickle.dump(best_model, output_file)\n",
    "\n",
    "# Save the linear model for comparison later\n",
    "with open(config.LOGISTIC_REGRESSION_MODEL, 'wb') as output_file:\n",
    "    pickle.dump(linear_model, output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
