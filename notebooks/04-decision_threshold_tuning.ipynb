{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diabetes prediction: decision threshold tuning\n",
    "\n",
    "## Notebook set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python standard library imports\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "# PyPI imports - data manipulation and visualization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# PyPI imports - machine learning libraries\n",
    "from sklearn.model_selection import cross_val_score, TunedThresholdClassifierCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Internal imports - project-specific modules\n",
    "import configuration as config\n",
    "import functions as funcs\n",
    "\n",
    "# Wait for the gradient boosting notebook to finish execution before proceeding\n",
    "# This ensures all required model files are available\n",
    "while True:\n",
    "    if os.path.exists(config.GRADIENT_BOOSTING_MODEL):\n",
    "        break\n",
    "    else:\n",
    "        time.sleep(5)  # Check every 5 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data preparation\n",
    "\n",
    "### 1.1. Load data from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessed dataset that was saved from the decision tree notebook\n",
    "with open(config.DATA_FILE, 'rb') as input_file:\n",
    "    dataset=pickle.load(input_file)\n",
    "\n",
    "# Extract training and testing dataframes from the loaded dictionary\n",
    "training_df = dataset['training']\n",
    "testing_df = dataset['testing']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model comparisons\n",
    "\n",
    "In this section, we compare the performance of all the machine learning algorithms we've trained and optimized in previous notebooks. This comprehensive comparison helps us identify the best-performing model before applying threshold tuning.\n",
    "\n",
    "### 2.1. Cross-validation accuracy\n",
    "\n",
    "**Why compare models systematically?**\n",
    "\n",
    "Before diving into threshold optimization, we need to establish which algorithm performs best with standard evaluation metrics. Each algorithm has different strengths and weaknesses:\n",
    "\n",
    "**Logistic Regression:**\n",
    "- **Strengths**: Simple, interpretable, fast training, good baseline\n",
    "- **Assumptions**: Linear relationship between features and log-odds\n",
    "- **Best for**: When interpretability is crucial, linearly separable data\n",
    "\n",
    "**Decision Tree:**\n",
    "- **Strengths**: Highly interpretable, handles non-linear relationships, no feature scaling needed\n",
    "- **Weaknesses**: Prone to overfitting, unstable (small data changes = different trees)\n",
    "- **Best for**: When you need to explain decisions, feature interactions matter\n",
    "\n",
    "**Random Forest:**\n",
    "- **Strengths**: Reduces overfitting, handles missing values, provides feature importance\n",
    "- **How it works**: Combines many decision trees with voting\n",
    "- **Best for**: Robust performance across various datasets, good default choice\n",
    "\n",
    "**Gradient Boosting:**\n",
    "- **Strengths**: Often highest accuracy, learns from mistakes sequentially\n",
    "- **How it works**: Builds models iteratively, each correcting previous errors\n",
    "- **Best for**: When accuracy is paramount, complex patterns in data\n",
    "\n",
    "**Cross-validation methodology:**\n",
    "- Uses k-fold cross-validation for robust performance estimates\n",
    "- Prevents overly optimistic results from single train-test splits\n",
    "- Provides confidence intervals (variance) in model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dictionary to store all trained models for comparison\n",
    "models = {}\n",
    "\n",
    "# Load the logistic regression model from previous notebook\n",
    "with open(config.LOGISTIC_REGRESSION_MODEL, 'rb') as input_file:\n",
    "    models['Logistic regression'] = pickle.load(input_file)\n",
    "\n",
    "# Load the best decision tree model from previous notebook\n",
    "with open(config.DECISION_TREE_MODEL, 'rb') as input_file:\n",
    "    models['Decision tree'] = pickle.load(input_file)\n",
    "\n",
    "# Load the best random forest model from previous notebook\n",
    "with open(config.RANDOM_FOREST_MODEL, 'rb') as input_file:\n",
    "    models['Random forest'] = pickle.load(input_file)\n",
    "\n",
    "# Load the best gradient boosting model from previous notebook\n",
    "with open(config.GRADIENT_BOOSTING_MODEL, 'rb') as input_file:\n",
    "    models['Gradient boosting'] = pickle.load(input_file)\n",
    "\n",
    "# Initialize dictionary to store cross-validation scores for comparison\n",
    "cross_val_scores = {\n",
    "    'Model': [],\n",
    "    'Score': []\n",
    "}\n",
    "\n",
    "# Perform cross-validation for each model and collect scores\n",
    "for model_name, model in models.items():\n",
    "    scores = cross_val_score(\n",
    "        model,\n",
    "        training_df.drop('Outcome', axis=1),\n",
    "        training_df['Outcome'],\n",
    "        cv=config.CROSS_VAL,        # Use stratified shuffle split for cross-validation\n",
    "        n_jobs=-1                   # Use all available CPU cores\n",
    "    )\n",
    "\n",
    "    # Store results for visualization\n",
    "    cross_val_scores['Model'].extend([model_name]*len(scores))\n",
    "    cross_val_scores['Score'].extend(np.array(scores)*100)\n",
    "\n",
    "# Create boxplot to compare cross-validation performance across all models\n",
    "plt.title(f'Cross-validation scores')\n",
    "sns.boxplot(pd.DataFrame.from_dict(cross_val_scores), x='Model', y='Score')\n",
    "plt.xlabel(model_name)\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Test set performance\n",
    "\n",
    "While cross-validation scores give us a robust estimate of model performance, the ultimate test is how well each model performs on completely unseen data - our held-out test set.\n",
    "\n",
    "**Why test set evaluation matters:**\n",
    "\n",
    "**Unbiased performance estimate:**\n",
    "- Test data was never seen during training or hyperparameter optimization\n",
    "- Provides realistic estimate of real-world performance\n",
    "- Reveals if models generalize well beyond training data\n",
    "\n",
    "**Confusion matrix insights:**\n",
    "- **True Positives (TP)**: Correctly identified diabetes cases - critical for patient health\n",
    "- **True Negatives (TN)**: Correctly identified healthy patients - avoids unnecessary anxiety\n",
    "- **False Positives (FP)**: Healthy patients misclassified as diabetic - leads to unnecessary tests\n",
    "- **False Negatives (FN)**: Diabetic patients missed - potentially dangerous for patient health\n",
    "\n",
    "**Medical context considerations:**\n",
    "- In healthcare, False Negatives are typically more costly than False Positives\n",
    "- Missing a diabetes diagnosis can lead to serious complications\n",
    "- A false alarm leads to additional testing but catches the condition if present\n",
    "- This asymmetric cost structure will influence our threshold optimization later\n",
    "\n",
    "**What to look for in confusion matrices:**\n",
    "- **Sensitivity (Recall)**: TP/(TP+FN) - ability to catch positive cases\n",
    "- **Specificity**: TN/(TN+FP) - ability to correctly identify negative cases  \n",
    "- **Precision**: TP/(TP+FP) - when model says positive, how often is it right?\n",
    "- **Overall patterns**: Which models show better balance between error types?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funcs.plot_confusion_matrices(models=models, testing_df=testing_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Decision threshold analysis\n",
    "\n",
    "### 3.1. Predicted class probabilities\n",
    "\n",
    "Understanding how well-calibrated our models are by examining their predicted probabilities:\n",
    "\n",
    "Before we can optimize decision thresholds, we need to understand how our models generate probability predictions. Well-calibrated models should produce probabilities that reflect the true likelihood of each class. For example, when a model predicts a 70% probability of diabetes, approximately 70% of such cases should actually have diabetes.\n",
    "\n",
    "By examining the distribution of predicted probabilities for each true class, we can assess:\n",
    "- **Model confidence**: How confident are the models in their predictions?\n",
    "- **Class separation**: How well do the models distinguish between the two classes?\n",
    "- **Calibration quality**: Are the predicted probabilities realistic estimates of true probabilities?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplot grid for threshold comparison (2 rows: probabilities + confusion matrices)\n",
    "fig, axs = plt.subplots(1,4, figsize=(12,3))\n",
    "axs = axs.flatten()\n",
    "\n",
    "fig.suptitle(f'Predicted class probability distributions')\n",
    "\n",
    "for i, (model_name, model) in enumerate(models.items()):\n",
    "\n",
    "    # Extract class probabilities from logistic regression model for threshold analysis\n",
    "    class_probabilities = model.predict_proba(testing_df.drop('Outcome', axis=1))\n",
    "\n",
    "    # Create dataframe with probabilities and actual labels for visualization\n",
    "    testing_probabilities_df = pd.DataFrame({\n",
    "        'Diabetes probability': class_probabilities[:,1],  # Probability of class 1 (diabetes)\n",
    "        'Label': testing_df['Outcome']\n",
    "    })\n",
    "\n",
    "    # Create human-readable outcome labels for plotting\n",
    "    testing_probabilities_df['Outcome'] = testing_probabilities_df['Label'].map({0: 'No diabetes', 1: 'Diabetes'})\n",
    "\n",
    "    sns.boxplot(testing_probabilities_df, x='Outcome', y='Diabetes probability', ax=axs[i])\n",
    "    axs[i].set_title(model_name)\n",
    "    axs[i].set_xlabel('')\n",
    "    axs[i].set_ylabel('Predicted diabetes probability')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Call threshold comparison  \n",
    "\n",
    "By default, classification models use a decision threshold of 0.5 - if the predicted probability is above 0.5, the model predicts the positive class. However, this threshold may not be optimal for all problems, especially with imbalanced datasets.\n",
    "\n",
    "**Why threshold tuning matters:**\n",
    "- **Medical diagnosis**: False negatives (missing diabetes cases) might be more costly than false positives (unnecessary follow-up tests)\n",
    "- **Class imbalance**: When one class is rare, the default 0.5 threshold often leads to poor recall for the minority class\n",
    "- **Business objectives**: Different applications may prioritize precision vs. recall differently\n",
    "\n",
    "**Key considerations:**\n",
    "- **Lower thresholds (e.g., 0.2)**: More sensitive, catch more positive cases but increase false alarms\n",
    "- **Higher thresholds (e.g., 0.7)**: More specific, fewer false alarms but might miss positive cases\n",
    "- **Cost-benefit analysis**: The optimal threshold depends on the relative costs of false positives vs. false negatives\n",
    "\n",
    "Let's examine how different thresholds affect the performance of our best model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define different decision thresholds to compare (default is 0.5)\n",
    "call_thresholds = [0.20, 0.30, 0.40, 0.5]\n",
    "class_probabilities = models['Gradient boosting'].predict_proba(testing_df.drop('Outcome', axis=1))\n",
    "\n",
    "# Create subplot grid for threshold comparison (2 rows: probabilities + confusion matrices)\n",
    "fig, axs = plt.subplots(2,4, figsize=(14,6))\n",
    "\n",
    "fig.suptitle(f'Gradient boosting: predicted diabetes probability & decision threshold')\n",
    "\n",
    "# For each threshold, create probability distribution plot and confusion matrix\n",
    "for i, threshold in enumerate(call_thresholds):\n",
    "\n",
    "    # Top row: Plot probability distributions with threshold line\n",
    "    sns.boxplot(testing_probabilities_df, x='Outcome', y='Diabetes probability', ax=axs[0,i])\n",
    "    axs[0,i].axhline(threshold, color='red', linestyle='dashed', linewidth=1)\n",
    "\n",
    "    # Label default threshold differently\n",
    "    if threshold == 0.5:\n",
    "        axs[0,i].set_title(f'Threshold: {threshold} (default)')\n",
    "    else:\n",
    "        axs[0,i].set_title(f'Threshold: {threshold}')\n",
    "\n",
    "    axs[0,i].set_xlabel('')\n",
    "    axs[0,i].set_ylabel('Predicted diabetes probability')\n",
    "\n",
    "    # Bottom row: Generate predictions using threshold and plot confusion matrix\n",
    "    calls = [1 if p > threshold else 0 for p in class_probabilities[:,1]]\n",
    "    accuracy = accuracy_score(calls, testing_df['Outcome'])*100\n",
    "\n",
    "    # Create normalized confusion matrix\n",
    "    cm = confusion_matrix(testing_df['Outcome'], calls, normalize='true')\n",
    "    cm_disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['healthy','diabetic'])\n",
    "\n",
    "    # Label with threshold and accuracy\n",
    "    if threshold == 0.5:\n",
    "        axs[1,i].set_title(f'Threshold: {threshold} (default)\\naccuracy: {accuracy:.2f}%')\n",
    "    else:\n",
    "        axs[1,i].set_title(f'Threshold: {threshold}\\naccuracy: {accuracy:.1f}%')\n",
    "\n",
    "    _ = cm_disp.plot(ax=axs[1,i])\n",
    "\n",
    "# Adjust layout to prevent overlapping\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Decision threshold optimization\n",
    "\n",
    "### 4.1. Threshold scan\n",
    "\n",
    "To find the optimal threshold, we need to understand how false positive and false negative rates change across different threshold values. This helps us make informed decisions about the trade-offs between different types of errors.\n",
    "\n",
    "**Understanding the error rate curves:**\n",
    "\n",
    "**False Positive Rate (FPR)**:\n",
    "- Formula: FP / (FP + TN)\n",
    "- Represents the fraction of healthy patients incorrectly classified as diabetic\n",
    "- Decreases as threshold increases (fewer false alarms)\n",
    "- In medical context: Unnecessary stress and follow-up tests for healthy patients\n",
    "\n",
    "**False Negative Rate (FNR)**:\n",
    "- Formula: FN / (FN + TP)  \n",
    "- Represents the fraction of diabetic patients incorrectly classified as healthy\n",
    "- Increases as threshold increases (more missed cases)\n",
    "- In medical context: Missed diagnoses that could lead to serious health complications\n",
    "\n",
    "**The optimization challenge:**\n",
    "- These error rates move in opposite directions as we adjust the threshold\n",
    "- The intersection point represents equal error rates\n",
    "- The optimal threshold depends on the relative costs and consequences of each error type\n",
    "- Medical applications typically prefer lower thresholds to minimize missed diagnoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplot grid to analyze error rates vs threshold for each model\n",
    "fig, axs = plt.subplots(1,4, figsize=(14,4))\n",
    "axs = axs.flatten()\n",
    "\n",
    "fig.suptitle(f'Error rates vs decision threshold')\n",
    "\n",
    "# Analyze each model's performance across different thresholds\n",
    "for i, (model_name, model) in enumerate(models.items()):\n",
    "\n",
    "    # Initialize dictionary to store error rates at different thresholds\n",
    "    error_rates = {\n",
    "        'Threshold': [],\n",
    "        'False positive rate': [],\n",
    "        'False negative rate': []\n",
    "    }\n",
    "\n",
    "    # Test thresholds from 0 to 1 with 100 points\n",
    "    call_thresholds = np.linspace(0, 1, 100)\n",
    "\n",
    "    # Get class probabilities for the current model\n",
    "    class_probabilities = model.predict_proba(testing_df.drop('Outcome', axis=1))\n",
    "\n",
    "    # Calculate error rates for each threshold\n",
    "    for j, threshold in enumerate(call_thresholds):\n",
    "        # Make predictions using current threshold\n",
    "        calls = [1 if p > threshold else 0 for p in class_probabilities[:,1]]\n",
    "        cm = confusion_matrix(testing_df['Outcome'], calls)\n",
    "\n",
    "        # Extract confusion matrix components: True Negative, False Positive, False Negative, True Positive\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "        # Calculate error rates\n",
    "        error_rates['False positive rate'].append(fp / (fp + tn))  # FPR = FP / (FP + TN)\n",
    "        error_rates['False negative rate'].append(fn / (fn + tp))  # FNR = FN / (FN + TP)\n",
    "        error_rates['Threshold'].append(threshold)\n",
    "\n",
    "    # Plot error rates for current model\n",
    "    axs[i].set_title(f'{model_name}')\n",
    "    axs[i].plot(error_rates['Threshold'], error_rates['False positive rate'], label='False positive rate')\n",
    "    axs[i].plot(error_rates['Threshold'], error_rates['False negative rate'], label='False negative rate')\n",
    "    axs[i].set_xlabel('Decision threshold')\n",
    "    axs[i].set_ylabel('Error rate')\n",
    "    axs[i].legend(loc='best')\n",
    "\n",
    "# Adjust layout to prevent overlapping\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Automated threshold tuning\n",
    "\n",
    "Instead of manually selecting thresholds, we can use scikit-learn's `TunedThresholdClassifierCV` to automatically find the optimal threshold using cross-validation. This approach optimizes for balanced accuracy, which is particularly useful for imbalanced datasets.\n",
    "\n",
    "**How TunedThresholdClassifierCV works:**\n",
    "\n",
    "1. **Cross-validation setup**: Uses k-fold CV to ensure robust threshold selection\n",
    "2. **Threshold search**: Tests multiple threshold values across the range [0, 1]\n",
    "3. **Scoring optimization**: Optimizes for balanced accuracy by default\n",
    "4. **Best threshold selection**: Chooses the threshold that maximizes the chosen metric\n",
    "\n",
    "**Why balanced accuracy?**\n",
    "- **Formula**: (Sensitivity + Specificity) / 2\n",
    "- **Sensitivity (Recall)**: TP / (TP + FN) - ability to correctly identify positive cases\n",
    "- **Specificity**: TN / (TN + FP) - ability to correctly identify negative cases\n",
    "- **Balance**: Gives equal weight to performance on both classes, preventing bias toward the majority class\n",
    "- **Imbalanced data**: More appropriate than standard accuracy when classes are unequal\n",
    "\n",
    "**Advantages of automated tuning:**\n",
    "- Removes manual guesswork and potential bias\n",
    "- Uses cross-validation for robust threshold selection\n",
    "- Consistent methodology across different models\n",
    "- Can easily switch optimization metrics based on problem requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplot grid to compare tuned threshold performance for each model\n",
    "fig, axs = plt.subplots(1,4, figsize=(14,4))\n",
    "axs = axs.flatten()\n",
    "\n",
    "fig.suptitle(f'Tuned model test set performance comparison')\n",
    "\n",
    "# Apply threshold tuning to each model\n",
    "for i, (model_name, model) in enumerate(models.items()):\n",
    "\n",
    "    # Create threshold-tuned version of the model using cross-validation\n",
    "    tuned_model = TunedThresholdClassifierCV(\n",
    "        model,\n",
    "        scoring='balanced_accuracy',     # Optimize for balanced accuracy (handles class imbalance)\n",
    "        cv=config.CROSS_VAL,\n",
    "        n_jobs=-1,                       # Use all available CPU cores\n",
    "        random_state=config.RANDOM_SEED  # Ensure reproducible results\n",
    "    )\n",
    "\n",
    "    # Fit the threshold tuner on training data to find optimal threshold\n",
    "    tuned_model.fit(training_df.drop('Outcome', axis=1), training_df['Outcome'])\n",
    "    \n",
    "    # Replace original model with tuned version in models dictionary\n",
    "    models[model_name] = tuned_model\n",
    "\n",
    "    # Make predictions on test set using optimized threshold\n",
    "    testing_predictions = tuned_model.predict(testing_df.drop('Outcome', axis=1))\n",
    "    accuracy = accuracy_score(testing_predictions, testing_df['Outcome'])*100\n",
    "\n",
    "    # Create normalized confusion matrix for tuned model\n",
    "    cm = confusion_matrix(testing_df['Outcome'], testing_predictions, normalize='true')\n",
    "    cm_disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    _ = cm_disp.plot(ax=axs[i])\n",
    "\n",
    "    # Display model name, optimal threshold, and accuracy\n",
    "    axs[i].set_title(f'{model_name}\\ntuned threshold: {tuned_model.best_threshold_:.2f}\\naccuracy: {accuracy:.1f}%')\n",
    "    axs[i].set_xlabel('Predicted outcome')\n",
    "    axs[i].set_ylabel('True outcome')\n",
    "\n",
    "# Adjust layout to prevent overlapping\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save final model\n",
    "\n",
    "### 5.1. Model persistence\n",
    "\n",
    "After comparing all algorithms and optimizing decision thresholds, we save the best performing model for future use. The threshold-tuned gradient boosting model represents our final solution for diabetes prediction.\n",
    "\n",
    "**Why this model is our final choice:**\n",
    "\n",
    "**Algorithm advantages:**\n",
    "- **Gradient boosting**: Sequential ensemble method that learns from previous mistakes\n",
    "- **Robustness**: Less prone to overfitting than individual decision trees\n",
    "- **Feature interactions**: Capable of capturing complex relationships in the data\n",
    "- **Performance**: Typically achieves high accuracy on tabular data\n",
    "\n",
    "**Threshold optimization benefits:**\n",
    "- **Balanced accuracy**: Optimized for equal performance on both classes\n",
    "- **Medical relevance**: Better suited for healthcare applications where missing positive cases is costly\n",
    "- **Cross-validation**: Threshold selected using robust validation methodology\n",
    "- **Generalization**: More likely to perform well on new, unseen data\n",
    "\n",
    "**Production readiness:**\n",
    "- **Preprocessing included**: KNN imputation for missing values is built into the pipeline\n",
    "- **Calibrated thresholds**: Decision boundary optimized for the specific problem\n",
    "- **Serialized model**: Saved as pickle file for easy deployment and reuse\n",
    "- **Reproducible**: All random states set for consistent results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config.FINAL_MODEL, 'wb') as output_file:\n",
    "    pickle.dump(models['Gradient boosting'], output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
