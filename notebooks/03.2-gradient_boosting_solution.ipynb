{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diabetes prediction: gradient boosting\n",
    "\n",
    "## Notebook set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook implements **Gradient Boosting** - a sequential ensemble method that builds trees iteratively, with each new tree correcting errors from previous ones. We import `HistGradientBoostingClassifier` (scikit-learn's modern, efficient implementation) and wait for the Random Forest notebook to complete first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python standard library imports\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# PyPI imports - data manipulation and visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# PyPI imports - statistical and machine learning libraries\n",
    "from scipy.stats import randint, uniform, loguniform\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import cross_val_score, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Internal imports - project-specific modules\n",
    "import configuration as config\n",
    "import functions as funcs\n",
    "\n",
    "# Wait for the random forest notebook to finish execution\n",
    "while True:\n",
    "    if os.path.exists(config.RANDOM_FOREST_MODEL):\n",
    "        break\n",
    "\n",
    "    else:\n",
    "        time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data loading\n",
    "\n",
    "### 1.1. Load data from disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We continue using the same preprocessed dataset for consistency across all algorithm comparisons. This ensures any performance differences are due to the algorithms themselves, not data preprocessing variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessed dataset that was saved from the decision tree notebook\n",
    "with open(config.DATA_FILE, 'rb') as input_file:\n",
    "    dataset = pickle.load(input_file)\n",
    "\n",
    "# Extract training and testing dataframes from the loaded dictionary\n",
    "training_df = dataset['training']\n",
    "testing_df = dataset['testing']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Inspect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick verification of the loaded dataset structure and contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model training\n",
    "\n",
    "### 2.1. Previous scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now implement Gradient Boosting, which differs from Random Forest by building trees sequentially rather than in parallel. Each new tree focuses on correcting the mistakes of previous trees, potentially leading to superior performance but with higher risk of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cross validation scores from previous models\n",
    "with open(config.CROSS_VAL_SCORES_FILE, 'rb') as input_file:\n",
    "    cross_val_scores = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the comprehensive performance results from all previous models (logistic regression, decision tree variants, Random Forest variants) to maintain our complete algorithm comparison database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Gradient boosting model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting builds an ensemble by sequentially adding weak learners (typically small trees) that correct the errors of previous models. Key characteristics:\n",
    "\n",
    "- **Sequential learning**: Each tree learns from previous trees' mistakes\n",
    "- **Error correction**: New trees focus on hard-to-predict samples\n",
    "- **Typically superior performance**: Often outperforms Random Forest\n",
    "- **Higher overfitting risk**: Sequential learning can memorize training data\n",
    "- **Slower training**: Cannot parallelize tree construction like Random Forest\n",
    "\n",
    "We use `HistGradientBoostingClassifier` for its efficiency and modern optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the list of features to impute\n",
    "with open(config.IMPUTED_FEATURES_FILE, 'rb') as input_file:\n",
    "    imputed_features = pickle.load(input_file)\n",
    "\n",
    "# Create the imputer\n",
    "knn_imputer = ColumnTransformer([('imputer', KNNImputer(), imputed_features)], remainder='passthrough')\n",
    "\n",
    "# Create a gradient boosting classifier with default hyperparameters\n",
    "tree_model = HistGradientBoostingClassifier(\n",
    "    class_weight=config.CLASS_WEIGHT,\n",
    "    random_state=config.RANDOM_SEED\n",
    ")\n",
    "\n",
    "naive_model = Pipeline(\n",
    "    steps=[\n",
    "        ('KNN', knn_imputer),\n",
    "        ('classifier', tree_model)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Train the naive gradient boosting model\n",
    "naive_model.fit(training_df.drop('Outcome', axis=1), training_df['Outcome'])\n",
    "\n",
    "# Calculate and display training accuracy\n",
    "accuracy = accuracy_score(naive_model.predict(training_df.drop('Outcome', axis=1)), training_df['Outcome'])*100\n",
    "print(f'Training accuracy of gradient boosting model: {accuracy:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation is especially important for Gradient Boosting to detect overfitting. We expect to see:\n",
    "- **Potentially higher average performance**: Gradient boosting often achieves the best results\n",
    "- **Possible higher variance**: Sequential learning can be sensitive to training data\n",
    "- **Training vs validation gap**: Important to monitor for overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-validation on the naive gradient boosting model\n",
    "scores = cross_val_score(\n",
    "    naive_model,\n",
    "    training_df.drop('Outcome', axis=1),\n",
    "    training_df['Outcome'],\n",
    "    cv=config.CROSS_VAL,\n",
    "    n_jobs=-1                   # Use all available CPU cores for parallel processing\n",
    ")\n",
    "\n",
    "# Store results for comparison with previous models and future optimized model\n",
    "cross_val_scores['Model'].extend(['Gradient boosting']*len(scores))\n",
    "cross_val_scores['Score'].extend(scores*100)\n",
    "\n",
    "# Display cross-validation results with mean and standard deviation\n",
    "print(f'Cross-validation accuracy: {np.mean(scores)*100:.1f} +/- {np.std(scores)*100:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hyperparameter optimization\n",
    "\n",
    "### 3.1. Hyperparameter search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting hyperparameter optimization includes unique parameters not found in Random Forest:\n",
    "\n",
    "- **Learning rate**: Controls how much each new tree contributes (lower = more conservative)\n",
    "- **Max iterations**: Number of sequential trees to build\n",
    "- **Regularization**: L2 penalty to prevent overfitting\n",
    "- **Histogram binning**: Efficiency optimization for continuous features\n",
    "- **Interaction constraints**: Control feature interactions between trees\n",
    "\n",
    "Proper tuning is crucial for Gradient Boosting to achieve optimal performance without overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Define hyperparameter search space for gradient boosting optimization\n",
    "hyperparameters = {\n",
    "    'KNN__imputer__n_neighbors': randint(1, 3),\n",
    "    'KNN__imputer__weights': ['uniform', 'distance'],\n",
    "    'KNN__imputer__add_indicator': [True, False],\n",
    "    'classifier__learning_rate': loguniform(10**-5, 1.0),\n",
    "    'classifier__max_iter': randint(2, 200),\n",
    "    'classifier__max_leaf_nodes':randint(2, 1000),\n",
    "    'classifier__max_depth':randint(1, 20),\n",
    "    'classifier__min_samples_leaf': randint(1, 20),\n",
    "    'classifier__l2_regularization': loguniform(10**-5, 1.0),\n",
    "    'classifier__max_features':uniform(loc=0.1, scale=0.9),\n",
    "    'classifier__max_bins': randint(10, 255),\n",
    "    'classifier__interaction_cst': ['pairwise', 'no_interactions'],\n",
    "}\n",
    "\n",
    "# Perform randomized search over hyperparameters\n",
    "search = RandomizedSearchCV(\n",
    "    naive_model,\n",
    "    hyperparameters,\n",
    "    return_train_score=True,                 # Return training scores for analysis\n",
    "    cv=config.CROSS_VAL,                     # Use stratified shuffle split for cross-validation\n",
    "    n_jobs=-1,                               # Use all available CPU cores\n",
    "    n_iter=config.RANDOM_SEARCH_ITERATIONS,  # Number of parameter combinations to try\n",
    "    random_state=config.RANDOM_SEED          # Ensure reproducible results\n",
    ")\n",
    "\n",
    "# Fit the search and extract best model and parameters\n",
    "search_results = search.fit(training_df.drop('Outcome', axis=1), training_df['Outcome'])\n",
    "best_model = search_results.best_estimator_\n",
    "winning_hyperparameters = search_results.best_params_\n",
    "\n",
    "# Display the best hyperparameters found\n",
    "print('Best hyperparameters:\\n')\n",
    "\n",
    "for key, value in search_results.best_params_.items():\n",
    "    print(f' {key}: {value}')\n",
    "\n",
    "print()\n",
    "\n",
    "print(f'Run time ({os.cpu_count()} CPUs):\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Hyperparameter optimization results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We visualize the hyperparameter search results to understand how much hyperparameters matter and whether we found good solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funcs.plot_cross_validation(search_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Cross-validation of optimized model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the optimized Gradient Boosting model will show whether hyperparameter tuning successfully improved performance while avoiding overfitting. This comparison completes our tree-based algorithm evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-validation on the optimized gradient boosting model\n",
    "scores = cross_val_score(\n",
    "    best_model,\n",
    "    training_df.drop('Outcome', axis=1),\n",
    "    training_df['Outcome'],\n",
    "    cv=config.CROSS_VAL,\n",
    "    n_jobs=-1                   # Use all available CPU cores for parallel processing\n",
    ")\n",
    "\n",
    "# Store results for comparison with all other models\n",
    "cross_val_scores['Model'].extend(['Optimized gradient boosting']*len(scores))\n",
    "cross_val_scores['Score'].extend(scores*100)\n",
    "\n",
    "# Display cross-validation results for the optimized gradient boosting model\n",
    "print(f'Cross-validation accuracy: {np.mean(scores)*100:.1f} +/- {np.std(scores)*100:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation\n",
    "\n",
    "### 4.1. Cross-validation performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final evaluation will determine if Gradient Boosting achieves the best performance among all tested algorithms. We'll compare against the logistic regression baseline and examine whether the sequential ensemble approach provides advantages over the parallel Random Forest approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create boxplot to compare cross-validation performance\n",
    "models = ['Logistic regression', 'Gradient boosting', 'Optimized gradient boosting']\n",
    "plot_scores = pd.DataFrame.from_dict(cross_val_scores)\n",
    "plot_scores = plot_scores[plot_scores['Model'].isin(models)]\n",
    "\n",
    "sns.boxplot(plot_scores, x='Model', y='Score')\n",
    "plt.title('Cross-validation performance comparison')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.xticks(rotation=25, ha='right')  # Rotate x-axis labels for better readability\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Focused comparison showing the progression from linear model to basic Gradient Boosting to optimized Gradient Boosting, highlighting the impact of the sequential ensemble method and hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Test set performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ultimate test: how do our Gradient Boosting models perform on completely unseen data? The confusion matrices will reveal:\n",
    "- **Sequential vs parallel ensemble**: Gradient Boosting vs Random Forest performance\n",
    "- **Overfitting assessment**: Whether optimization led to better generalization\n",
    "- **Clinical relevance**: How well each model identifies diabetic patients in practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain both naive and optimized gradient boosting models on the full training set\n",
    "result = naive_model.fit(training_df.drop('Outcome', axis=1), training_df['Outcome'])\n",
    "result = best_model.fit(training_df.drop('Outcome', axis=1), training_df['Outcome'])\n",
    "\n",
    "# Load the logistic regression model for comparison\n",
    "with open(config.LOGISTIC_REGRESSION_MODEL, 'rb') as input_file:\n",
    "    linear_model = pickle.load(input_file)\n",
    "\n",
    "# Generate confusion matrices for both gradient boosting models on the test set\n",
    "funcs.plot_confusion_matrices(\n",
    "    models = {\n",
    "        'Logistic regression': linear_model,\n",
    "        'Gradient boosting': naive_model,\n",
    "        'Optimized gradient boosting': best_model\n",
    "    },\n",
    "    testing_df=testing_df\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save\n",
    "\n",
    "### 6.1. Cross-validation scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the final Gradient Boosting results to complete our comprehensive algorithm comparison database, preserving all model performances for the ultimate project evaluation and potential deployment of the best-performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config.CROSS_VAL_SCORES_FILE, 'wb') as output_file:\n",
    "    pickle.dump(cross_val_scores, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final update to our cross-validation scores database with Gradient Boosting results, completing the comprehensive performance comparison across all tested algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the optimized Gradient Boosting model and hyperparameters for:\n",
    "- **Final comparison**: Loading in other notebooks for comprehensive evaluation\n",
    "- **Deployment readiness**: The model can be used for production predictions\n",
    "- **Research reproduction**: Exact configurations can be recreated\n",
    "- **Documentation**: Complete record of the best-performing sequential ensemble approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the optimal hyperparameters for future use or reproduction\n",
    "with open(config.GRADIENT_BOOSTING_HYPERPARAMETERS, 'wb') as output_file:\n",
    "    pickle.dump(winning_hyperparameters, output_file)\n",
    "\n",
    "# Save the trained best gradient boosting model for deployment or further analysis\n",
    "with open(config.GRADIENT_BOOSTING_MODEL, 'wb') as output_file:\n",
    "    pickle.dump(best_model, output_file)  # Fixed: should be best_model, not model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
